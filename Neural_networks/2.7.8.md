Исследовательская задача на понимание.

*Часть 1* 
При выстановлении ползунка на bias = 0, mixing = 0, shift = 0 мы получаем следующий график нашей целевой функции:
помимо глобального минимума с $J(w_1, w_2) < 0.1$ ближе к $0.0$, мы имеем как минимум два плато, одно примерно на $J(w_1, w_2) = 0.2$ и второе с $J(w_1, w_2)$ чуть меньше чем $0.3$, на которых наш алгоритм может застрять.

[3д рисунок]

Посмотрим, что же получится, при изменении learning rate (lr). Начнем с $lr=0.01$, на графике с кривой обучения мы видим, что наша ошибка монотонно убывает и наш алгоритм за 2000 итераций не успевает найти минимум и застревает на $J(w_1, w_2)$ примерно $0.2090$, при данном раскладе мы хотели бы увеличить lr.

[рисунок кривой обучения 0.01]

Посмотрим, что будет если увеличить lr до 0.05: мы  видим, что до 1000 итераций уменьшение ошибки происходит более стремительно, чем после. Но даже и после 1000 итераций, наша $J(w_1, w_2)$ монотонно убывает и с lr на 0.05, наш алгоритм останавливается после 2000 итераций не достигая минимума на примерно $0.2060$. Из чего следует, что по сравению с 0.01 мы улучшили результат, но он все еще не оптимален.

[рисунок кривой обучения 0.05]

Посмотрим, что будет если увеличить lr до 0.1: похожая ситуация как с 0.05 уменьшение ошибки происходит, хоть и не так стремительно как с 0.01, алгоритм остановился после 2000 итераций на примерно $0.2050$ не достиг минимума.

[рисунок кривой обучения 0.1]


Посмотрим, что будет если увеличить lr до 0.5: похожая ситуация как с 0.05 уменьшение ошибки происходит, хоть и не так стремительно как с 0.01, алгоритм остановился после 2000 итераций на примерно $0.2040$ не достиг минимума.

[рисунок кривой обучения 0.5]

Посмотрим, что будет если увеличить lr до 1: мы видим больше вариации а J не такие плавные шаги как в предыдущих вариантах, но все-таки функия $J$ убывает, но не уменьшается меньше чем $0.2$

[рисунок кривой обучения 1]


Можно предположить, что наш алгоритм с lr в пределах [0.01, 1] застревают где-то на плато и величина градиента в этих районах не очень большая и имея маленький шаг нашему алгоритму не получается выбраться с этих плато и они останавливаются после 2000 итераций.


Что же будет, если мы увеличим lr до 5: Мы видим как наша кривая обучения застревает сразу же в начале обучения на 0.2, как раз где находится одно из двух плато, но за около 1500 итерций нашему алгоритму удается вылезти с этого плато и двигаться в сторону минимума на $0.08$. Мы видим, что наш алгоритм с более высокой lr смог дстичь более хорошего результата 

[рисунок кривой обучения 5]

Но что-же получится, если мы еще увеличим lr до 10: вышло относительно похоже, как и в случае с 5 - мы сначала застряли на плато, потом выбрались из него и уже смогли двигаться в сторону глобального минимума. Так же мы смогли увидеть, что при более высоком lr перепады $J$ между  шагами гораздо сильнее, чем при маленьких величинах lr.

[рисунок кривой обучения 10]

Из этого эксперимента можно было бы подумать, что всегда хорошо иметь более высокую lr, на на самом деле не так, имея слишком большй шаг, мы можем начать перескакивать через наш глобальный или локальный минимум и никогда не достигать его дна. Поэтому как ответ для *Часть 2* мне кажется целесобразным предложить алгоритм который имеет возможность адаптировать lr: например сначала имеет чуть более высокую lr, которая постиенно будет снижаться


